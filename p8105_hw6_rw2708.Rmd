---
title: "hw6"
author: "Ran Wang"
date: "11/21/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(tidyverse)
library(rvest)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r probelm1}
birthweight = read.csv("./datahw6/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(babysex= as.factor(babysex),
         babysex= recode(babysex,"1"="male","2"="female"),
         frace = as.factor(frace),
         frace = recode(frace,"1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
         mrace = as.factor(mrace),
         mrace = recode(mrace,"1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
         malform = as.factor(malform),
         malform = recode(malform,"0" = "absent", "1" = "present")
         )

```


The first step is to load and clean the data for regression analysis. Some numeric variables are converted to factor with ordered levels. And there are no missing values in the dataset. 


```{r probelm1_backwise_selection}
fullmodel <- lm(bwt ~ ., data = birthweight) 
summary(fullmodel)

step(fullmodel,direction = "backward") 

#final model
linear_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight) 

summary(linear_model)
```

The backward (step-dwon) selection is applied to generate an ideal regression model for birthweight.The model starts with all candidate variables listed in the dataset.The significant level is set at 0.05. At each step of selection, the variable that is the least significant is removed. This process continues until no nonsignificant variables remain. In R, the output shows the value of Akaike information criterion (AIC) at each step, the goal of backward elimination is to select the model with lowest AIC. According to R output, the birthweight is best to be predicted by variables including babysex,bhead, blength, delwt, fincome,gaweeks, mheight, mrace, parity, ppwt and smoking status. This final model has the lowest AIC which is 48705.38.


```{r probelm1_residuals_prediction}
#calculate the residual and predicted value
residual = modelr::add_residuals(birthweight, linear_model)
prediction = modelr::add_predictions(birthweight, linear_model)

#join the above two datasets
residual_prediction = full_join(residual,prediction)

#show a plot of model residuals against fitted values 
plot1 = residual_prediction %>% 
   ggplot(aes(x = pred, y = resid,color = blength)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Residual vs. Prediction",
    x = "Predicted Value",
    y = "Residual") +
  geom_hline(aes(yintercept = 0), color = "black")
  
plot1
```

The above plot shows that model residuals against fitted values. As shown in the graph, the majority of residuals are around the 0-axis, which indicates the model is a good fit in describing the relationship between x and y. 


Comparing the model generated by backward selection and the following two models:

1),One using length at birth and gestational age as predictors (main effects only)
2),One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r probelm1_model_comparision}
cv_df =
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

#calculate the rmse of these three models
cv_df = 
  cv_df %>% 
  mutate(backward_model  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         model_1  = map(train,~lm(bwt ~ blength + gaweeks, data = .x)),
         model_2  = map(train, ~lm(bwt ~ bhead * blength + bhead * babysex + babysex * blength + bhead * babysex * blength, data = .x))) %>%
  mutate(rmse_backward = map2_dbl(backward_model, test, ~rmse(model = .x, data = .y)),
         rmse_model_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
         rmse_model_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y))
         )
  
cv_df  

#draw a violin plot to compare the rmse of three models
plot2 = cv_df  %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_jitter(shape = 10,  alpha = 0.35) 

plot2
```


As shown in the violin plot, the model that generated by the backward selection has the lowest rmse. The model using length at birth and gestational age as predictors (main effects only) has the highest rmse, which means length at birth and gestational age might not be good predictors for baby's birthweight. Thus, the model generated by backward selection is the best model to describe the relationship between outcome and predictors. In other words, predictors in the model can best predict the birthweight. 


## Probelm 2

```{r p2_r2_plot}
#read the weather dataset
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

#Use a 5000-bootstrap sample to produce the estimnated r square
bootstrap_sample = weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
        models = map(strap, ~lm(tmax ~ tmin, data = .x)),
        results = map(models, broom::tidy),
        var = map(models, broom::glance)
        ) %>% 
  select(-strap, -models) %>% 
  unnest(results,var)

#Plot the distribution of estimnated r square
plot_r_squared = bootstrap_sample %>% 
  filter(term == "tmin") %>% 
  ggplot(aes(x = r.squared)) + 
  geom_density(aes(y = stat(count / sum(count)))) +
  labs(
    title = "Density Plot of Estimated R-square",
    x = "The estimated R-square",
    y = "Density") +
  geom_vline(aes(xintercept = mean(r.squared)))

plot_r_squared 
```

As shown on the plot above, the distribution of estimated R-square might be approximately normal,with a black line indicating the mean value of estimated R-square.


``````{r p2_r2_ci}
#Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for estimated r square
ci_r_squared = bootstrap_sample %>% 
  filter(term == "tmin") %>% 
  pull(var = r.squared) %>% 
  quantile(c(0.025, 0.975))

ci_r_squared %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

The 95% confidence interval for estimated R-square is between 0.893 and 0.927.



```{r p2_log_plot}
#Use a 5000-bootstrap sample to produce the estimated log(beta0_hat * beta1_hat)
bootstrap_sample2 = bootstrap_sample %>% 
  select(.id, term, estimate) %>% 
  pivot_wider(
    id_cols = .id,
    names_from = term,
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(log = log(intercept * tmin))

#Plot the distribution of estimated log(beta0_hat * beta1_hat)
plot_log = bootstrap_sample2 %>% 
  ggplot(aes(x = log)) + 
  geom_density(aes(y = stat(count / sum(count)))) +
  labs(
    title = "Density Plot of Estimated log(beta0_hat * beta1_hat)",
    x = "log(beta0_hat * beta1_hat)",
    y = "Density") +
   geom_vline(aes(xintercept = mean(log)))

plot_log
```

As shown on the plot above, the distribution of estimated log(beta0_hat * beta1_hat) might be approximately normal, with a black line indicating the mean value of estimated log(beta0_hat * beta1_hat). Compared to the distribution of estimated R-square, it is slightly left skewed, which suggests that there are more lower values than higher values.  

```{r p2_log_ci}
#Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for estimated log(beta0_hat * beta1_hat)
ci_log = bootstrap_sample2 %>% 
  pull(log) %>% 
  quantile(c(0.025, 0.975))

ci_log %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```


The 95% confidence interval for estimated R-square is between 1.966 and 2.058.
